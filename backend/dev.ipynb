{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib.parse import urlparse, unquote\n",
    "import urllib3\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from bisect import bisect_right\n",
    "\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlObject():\n",
    "    def __init__(self, url):\n",
    "\n",
    "        if unquote(url) != url:\n",
    "            url = unquote(url)\n",
    "\n",
    "        self.raw_url = url\n",
    "        self.url = url.rstrip(\"/\")\n",
    "\n",
    "        url_parsed = urlparse(self.url)\n",
    "\n",
    "        self.hostname = url_parsed.hostname\n",
    "        self.path = url_parsed.path\n",
    "        self.query = url_parsed.query\n",
    "        self.protocol = url_parsed.scheme\n",
    "\n",
    "        self.split_path = [i for i in self.path.split(\"/\") if i]\n",
    "        self.path_params = self.path + \"?\" + self.query\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.hostname == other.hostname and self.path == other.path)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.path < other.path\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.url)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.url\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.url\n",
    "\n",
    "\n",
    "\n",
    "def getcontent(url, title):\n",
    "\n",
    "    chaps = []\n",
    "\n",
    "    for page in range(1, 20): #nothing should need more than 20 pages\n",
    "\n",
    "        \n",
    "        url_page = url.url + f\"?page={page}\" if not url.query else url.url + f\"&page={page}\"\n",
    "    \n",
    "        response = http.request('GET', url.url if page == 1 else url_page)\n",
    "\n",
    "        return_url = UrlObject(response.geturl())\n",
    "\n",
    "        print(url_page, return_url.url)\n",
    "\n",
    "        if response.status != 200 or not response.data:\n",
    "            break\n",
    "\n",
    "        if page!= 1 and url_page != return_url.url:\n",
    "            break #no more pages\n",
    "        \n",
    "        links = BeautifulSoup(response.data, 'html.parser', parse_only=SoupStrainer('a'), )\n",
    "\n",
    "        \n",
    "        if title == \"youtube\":\n",
    "            chaps.append([i for i in chaps if \"watch\" in i])\n",
    "            continue\n",
    "        \n",
    "        for link in links:\n",
    "            link_str = link.get(\"href\")\n",
    "\n",
    "            if  link_str and \"\" in link_str.lower():\n",
    "\n",
    "                parsed_link = UrlObject(link_str)\n",
    "\n",
    "                if parsed_link.hostname is None:\n",
    "                    parsed_link.hostname = url.hostname\n",
    "                    parsed_link.url =  url.hostname + link_str\n",
    "\n",
    "                if not parsed_link.protocol:\n",
    "                    parsed_link.protocol = url.protocol\n",
    "                    parsed_link.url = url.protocol + \"://\" + link_str\n",
    "\n",
    "                chaps.append(parsed_link)\n",
    "\n",
    "\n",
    "    return chaps\n",
    "\n",
    "def similarity(a, b):\n",
    "\n",
    "    score = 0\n",
    "    sp_a = a.split_path\n",
    "    sp_b = b.split_path\n",
    "\n",
    "    arr_len = min(len(sp_a), len(sp_b))\n",
    "    for path in range(arr_len):\n",
    "        str_len = min(len(sp_a[path]), len(sp_b[path]))\n",
    "        for char in range(min(len(sp_a[path]), len(sp_b[path]))):\n",
    "            if  sp_a[path][char] == sp_b[path][char]:\n",
    "                score += (arr_len - path) * (str_len - char) # early matches are more important\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return score\n",
    "\n",
    "def page_content(url):\n",
    "\n",
    "    response = http.request('GET', url.url)\n",
    "\n",
    "    if response.status == 404:\n",
    "        return False\n",
    "    \n",
    "    len_content = len(BeautifulSoup(response.data, 'html.parser', parse_only=SoupStrainer('p') )) #TODO remove <p> tags for comments\n",
    "\n",
    "    if len_content < 30: #TODO base this on size of previous content\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://fourseasonsforest.wordpress.com/about-your-pride-and-my-prejudice?page=1 https://fourseasonsforest.wordpress.com/about-your-pride-and-my-prejudice\n",
      "https://fourseasonsforest.wordpress.com/about-your-pride-and-my-prejudice?page=2 https://fourseasonsforest.wordpress.com/about-your-pride-and-my-prejudice\n",
      "https://lorenovels.com/surviving-in-a-romance-fantasy-novel?page=1 https://lorenovels.com/surviving-in-a-romance-fantasy-novel\n",
      "https://lorenovels.com/surviving-in-a-romance-fantasy-novel?page=2 https://lorenovels.com\n",
      "https://asuracomic.net/manga/magic-academys-genius-blinker?page=1 https://asuracomic.net/manga/magic-academys-genius-blinker\n",
      "https://asuracomic.net/manga/magic-academys-genius-blinker?page=2 https://asuracomic.net/manga/magic-academys-genius-blinker\n",
      "https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=1 https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters\n",
      "https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=2 https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=2\n",
      "https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=3 https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=3\n",
      "https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=4 https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters?page=4\n",
      "https://genesistls.com/series/the-academys-weakest-became-a-demon-limited-hunter?page=1 https://genesistls.com/series/the-academys-weakest-became-a-demon-limited-hunter\n",
      "https://genesistls.com/series/the-academys-weakest-became-a-demon-limited-hunter?page=2 https://genesistls.com/series/the-academys-weakest-became-a-demon-limited-hunter\n"
     ]
    }
   ],
   "source": [
    "examples = [(\"https://fourseasonsforest.wordpress.com/about-your-pride-and-my-prejudice/\", \"https://fourseasonsforest.wordpress.com/2021/12/14/about-your-pride-and-my-prejudice-01/\"),\n",
    "            (\"https://lorenovels.com/surviving-in-a-romance-fantasy-novel/\", \"https://lorenovels.com/chapter-59-black-moon-unit-part-6/\"),\n",
    "            (\"https://asuracomic.net/manga/magic-academys-genius-blinker/\", \"https://asuracomic.net/magic-academys-genius-blinker-chapter-22/\"), \n",
    "            (\"https://www.lightnovelworld.co/novel/advent-of-the-three-calamities/chapters\", \"https://www.lightnovelworld.co/novel/advent-of-the-three-calamities-1678/chapter-4\"),\n",
    "            (\"https://genesistls.com/series/the-academys-weakest-became-a-demon-limited-hunter/\", \"https://genesistls.com/demon-limited-hunter-chapter-1/\")]\n",
    "links = []\n",
    "\n",
    "for example in examples:\n",
    "\n",
    "    site = UrlObject(example[0])\n",
    "    title = \"\"\n",
    "\n",
    "    links.append(getcontent(site, title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(arr, values,  cond):\n",
    "    return [val for (i, val) in enumerate(arr) if cond(values[i])]\n",
    "\n",
    "def remove_duplicates(seq):\n",
    "    #keep last duplicate in list because links are grouped at end\n",
    "    seq = seq[::-1]\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "        \n",
    "    return result[::-1]\n",
    "\n",
    "def filter_links(target_link, links):\n",
    "   \n",
    "    link_sim = [(link, similarity(target_link, link)) for link in links]\n",
    "\n",
    "    scores = [sim for _, sim in link_sim]\n",
    "\n",
    "    #if all scores are the same (zscore fails in this case)\n",
    "\n",
    "    if len(set(scores)) == 1:\n",
    "            return links\n",
    "\n",
    "    z = stats.zscore(scores)\n",
    "    mean_z = np.mean(z)\n",
    "\n",
    "    link_sim = mask(link_sim, z, lambda x: x > mean_z)\n",
    "\n",
    "    link_sim = remove_duplicates(link_sim)\n",
    "\n",
    "    return link_sim\n",
    "\n",
    "def mean_filter(links):\n",
    "\n",
    "    scores = [sim for _, sim in links]\n",
    "    mean_s = np.mean(scores)\n",
    "    links = mask(links, scores, lambda x: x > 0.8 * mean_s)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "filtered_links = []\n",
    "scores = []\n",
    "\n",
    "for (i, example) in enumerate(examples):\n",
    "\n",
    "    res = filter_links(UrlObject(example[1]), links[i])\n",
    "    res = mean_filter(res)\n",
    "\n",
    "    filtered_links.append([res[i][0] for i in range(len(res))])\n",
    "    scores.append([res[i][1] for i in range(len(res))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find increasing/decreasing numbering\n",
    "\n",
    "def find_numbering(link):\n",
    "\n",
    "    path = link.path_params\n",
    "\n",
    "    numbers = set()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(path)):\n",
    "        if path[i].isdigit():\n",
    "            counter += 1\n",
    "        elif counter > 0:\n",
    "            numbers.add(int(path[i - counter:i]))\n",
    "            counter = 0\n",
    "\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def numbering(links):\n",
    "    numbers_arr = []\n",
    "    \n",
    "    for link in links:\n",
    "        numbers = find_numbering(link)\n",
    "        if numbers:\n",
    "            numbers_arr.append(numbers)\n",
    "\n",
    "\n",
    "    rev_numbers = numbers_arr[::-1]\n",
    "\n",
    "    a = longest_incrementing_subsequence(numbers_arr)\n",
    "    b = longest_incrementing_subsequence(rev_numbers)\n",
    "    return a  > b \n",
    "\n",
    "\n",
    "#lis for list of list of numbers\n",
    "\n",
    "def longest_incrementing_subsequence(nums):\n",
    "\n",
    "    if not nums:\n",
    "        return 0\n",
    "    \n",
    "    max_len = max([len(num_set) for num_set in nums])\n",
    "\n",
    "    nums = [sorted(list(num_set)) for num_set in nums]\n",
    "\n",
    "    dp_1 = [0]* max_len * len(nums)\n",
    "    dp_2 = [0]* max_len * len(nums)\n",
    "\n",
    "    for i in range(1, len(nums)):\n",
    "\n",
    "        pos = bisect_right(nums[i], nums[i-1][0])-1\n",
    "\n",
    "        for j in range(pos, len(nums[i])):\n",
    "            \n",
    "            j_pos = bisect_right(nums[i-1], nums[i][j])\n",
    "            dp_1[j] = 1 + dp_2[j_pos]\n",
    "        \n",
    "        dp_2 = dp_1\n",
    "        dp_1 = [0]* max_len * len(nums) \n",
    "\n",
    "    return max(dp_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
